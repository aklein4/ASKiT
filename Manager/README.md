# Manager

## Goal

This model will manage what actions we take given our current question/answer and evidence. This will be phrased as an RL agent operating in the following MDP:
 - State is defined as the current question and set of evidence
 - Action is a fact from the corpus that we could add to the evidence, a question that could be used to collect evidence, or a 'respond' action that gives the evidence to Responder for the final answer.
 - Reward: A discounted terminal reward for whether Responder got the answer right, and a small negative reward for each sub-question answered (because that is an expensive operation).
 
 ## Research
 
 SQUAD 2.0 has 'unanswerable' questions, so we might be able to train this to determine whether a question is answerable. This needs more research.
 
 ## Notes
 
  - Training should be jump-started with imitation of the known evidence facts in HotpotQA.
  - The order that evidence is collected could be a major factor, since this model will essentially be creating 'reasoning chains'.
  - There should be some iterative training with Responder getting trained to use the facts generated by this model, and this model getting retrained accordingly.
  - Should set a max-depth limit for question recursion, since the total number of facts collected scales as n**d where n is the number of facts per sub-question and d is the recursive depth (assuming that every fact is generated using a question)
  - Obviously we can't train using sub-questions as actions until after Asker has been trained. Technically sub-question actions are not needed anyway, as we could just evaluate the fact that comes out of the sub-question, but that is expensive.